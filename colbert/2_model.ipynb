{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from datetime import datetime\n",
    "\n",
    "import polars as pl\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "import einops\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import Module\n",
    "from torch import Tensor\n",
    "from polars import DataFrame\n",
    "from loguru import logger\n",
    "from lightning.pytorch.callbacks import RichProgressBar, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_DEBUG = True\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\" if IS_DEBUG else \"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"colbert/configs.yaml\"\n",
    "\n",
    "with open(path) as file:\n",
    "    conf = yaml.safe_load(file)\n",
    "\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(conf['PATHS']['tokenizer'])\n",
    "\n",
    "TOK_ID_MASK, TOK_ID_PAD = tokenizer.convert_tokens_to_ids(['[MASK]', '[PAD]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pl.read_parquet(conf['PROCESSED']['query'])\n",
    "queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw = Path(conf['PROCESSED']['corpus'].replace(\"[i]\", '*'))\n",
    "paths = Path(path_raw.parent).glob(path_raw.stem)\n",
    "\n",
    "if IS_DEBUG:\n",
    "    logger.debug(\"IS_DEBUG: Load part of corpus\")\n",
    "\n",
    "    corpus = pl.read_parquet(list(paths)[0])\n",
    "else:\n",
    "    logger.debug(\"Load full corpus\")\n",
    "\n",
    "    corpus = pl.concat([pl.read_parquet(path) for path in paths])\n",
    "\n",
    "corpus = corpus.with_columns(pl.col('did').cast(pl.Int64))\n",
    "\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "model_name = \"ColBERT\"\n",
    "\n",
    "with open(conf['PATHS']['punctuations']) as file:\n",
    "    map_punct2ids = json.load(file)\n",
    "\n",
    "punctuations = set(map_punct2ids.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataset and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_pairs(split: str) -> DataFrame:\n",
    "    \"\"\"Load positive pairs from train/val/tes split\n",
    "\n",
    "    Args:\n",
    "        split (str): split\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: positive pairs\n",
    "    \"\"\"\n",
    "\n",
    "    assert split in ['train', 'val', 'test']\n",
    "    assert Path(conf['RAW_DATA'][split]).exists()\n",
    "\n",
    "    pairs = (\n",
    "        pl\n",
    "        .read_csv(conf['RAW_DATA']['train'], separator='\\t')\n",
    "        .select(\n",
    "            pl.col('query-id').alias('qid'),\n",
    "            pl.col('corpus-id').alias('did')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return pairs\n",
    "\n",
    "# pairs = _load_pairs('train')\n",
    "# pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: str,\n",
    "        queries: DataFrame,\n",
    "        corpus: DataFrame,\n",
    "        punctuations: set,\n",
    "        col_query_id: str = \"qid\",\n",
    "        col_corpus_id: str = \"did\",\n",
    "        col_tok_ids: str = \"tok_ids\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.split = split\n",
    "        self.punctuations = punctuations\n",
    "        self.queries = queries\n",
    "        self.col_query_id = col_query_id\n",
    "        self.col_corpus_id = col_corpus_id\n",
    "        self.col_tok_ids = col_tok_ids\n",
    "\n",
    "        pairs = _load_pairs(split)\n",
    "\n",
    "        # Reducing the size of corpus and positive pairs\n",
    "        self.corpus = corpus.join(\n",
    "            pairs.select(col_corpus_id).unique(), on=col_corpus_id, how=\"inner\"\n",
    "        )\n",
    "        self.pairs = pairs.join(corpus, on=col_corpus_id, how=\"inner\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        entry = self.pairs[index]\n",
    "        qid, did = entry[self.col_query_id].item(), entry[self.col_corpus_id].item()\n",
    "\n",
    "        tok_ids_query = (\n",
    "            self.queries\n",
    "            .filter(pl.col(self.col_query_id) == pl.lit(qid))[self.col_tok_ids]\n",
    "            .item()\n",
    "            .to_numpy()\n",
    "            .copy()\n",
    "        )  # fmt: skip\n",
    "        tok_ids_doc = (\n",
    "            self.corpus\n",
    "            .filter(pl.col(self.col_corpus_id) == did)[self.col_tok_ids]\n",
    "            .item()\n",
    "            .to_numpy()\n",
    "            .copy()\n",
    "        )  # fmt: skip\n",
    "        mask = torch.tensor([tok in self.punctuations or tok == 0 for tok in tok_ids_doc])\n",
    "\n",
    "        attention_mask_query = (tok_ids_query != TOK_ID_MASK).astype(np.int32)\n",
    "        attention_mask_doc = (tok_ids_doc != TOK_ID_PAD).astype(np.int32)\n",
    "\n",
    "        return {\n",
    "            \"query\": tok_ids_query,\n",
    "            \"attention_mask_query\": attention_mask_query,\n",
    "            \"attention_mask_doc\": attention_mask_doc,\n",
    "            \"doc\": tok_ids_doc,\n",
    "            \"mask\": mask,\n",
    "        }\n",
    "\n",
    "\n",
    "# data = Data(\"train\", queries, corpus, punctuations)\n",
    "# # data[123]\n",
    "# loader = DataLoader(data, batch_size=int(conf[\"BSZ\"]), shuffle=True)\n",
    "# for batch in loader:\n",
    "#     break\n",
    "\n",
    "# batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAKS_VAL = -1e10\n",
    "\n",
    "class ColBERT(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model: str,\n",
    "        size_vocab: int,\n",
    "        d_hid: int = 128,\n",
    "        d_hid_bert: int = 768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.bert.resize_token_embeddings(size_vocab)\n",
    "\n",
    "        self.linear = nn.Linear(d_hid_bert, d_hid, bias=False)\n",
    "\n",
    "    def forward(self, X: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        # X: [bz, n]\n",
    "\n",
    "        X = self.bert(X, attention_mask).last_hidden_state\n",
    "        # [bz, n, d_hid_bert]\n",
    "\n",
    "        X = self.linear(X)\n",
    "        # [bz, n, d_hid]\n",
    "\n",
    "        # X = X / X.norm(dim=-1, keepdim=True)\n",
    "        X = torch.nn.functional.normalize(X, p=2, dim=2)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def trigger_train(self,\n",
    "            query: Tensor,\n",
    "            doc: Tensor,\n",
    "            mask: Tensor,\n",
    "            attention_mask_query: Tensor,\n",
    "            attention_mask_doc: Tensor,\n",
    "        ) -> Tensor:\n",
    "        # query: [bz, Nd]\n",
    "        # doc, mask: [bz, L]\n",
    "\n",
    "        bz, Nd = query.shape\n",
    "\n",
    "        ###################################################\n",
    "        # Encode query and document\n",
    "        ###################################################\n",
    "        query = self.forward(query, attention_mask_query)\n",
    "        # [bz, Nd, d_hid]\n",
    "        doc = self.forward(doc, attention_mask_doc)\n",
    "        # [bz, L, d_hid]\n",
    "\n",
    "\n",
    "        ###################################################\n",
    "        # Calculate the similarity\n",
    "        ###################################################\n",
    "        # Apply in-batch negative sampling\n",
    "        query = einops.repeat(query, \"b n d -> b repeat n d\", repeat=bz)\n",
    "\n",
    "        doc = einops.repeat(doc, \"b l d -> repeat b l d\", repeat=bz)\n",
    "        doc = einops.rearrange(doc, \"b a l d -> b a d l\")\n",
    "\n",
    "        sim = einops.einsum(query, doc, \"b a n d, b a d l -> b a n l\")\n",
    "        # [bz, bz, Nd, L]\n",
    "\n",
    "        # Mask positions which are the punctuation\n",
    "        mask = einops.repeat(mask, \"b l -> repeat1 b repeat2 l\", repeat1=bz, repeat2=Nd)\n",
    "        sim = sim.masked_fill(mask, MAKS_VAL)\n",
    "        \n",
    "        # Calculate score\n",
    "        score = (sim.max(dim=-1).values).sum(dim=-1)\n",
    "        # [bz, bz]\n",
    "\n",
    "\n",
    "        # Calculate Listwise CE\n",
    "        tgt = torch.arange(bz, dtype=torch.long, device=score.device)\n",
    "        # [bz]\n",
    "\n",
    "        loss = nn.functional.cross_entropy(score, tgt)\n",
    "\n",
    "        return score, loss\n",
    "\n",
    "# model = ColBERT(conf['MODEL_NAME'], len(tokenizer))\n",
    "# data = Data(\"train\", queries, corpus, punctuations)\n",
    "\n",
    "# loader = DataLoader(data, batch_size=int(conf[\"BSZ\"]), shuffle=True)\n",
    "# for batch in loader:\n",
    "#     score, loss = model.trigger_train(**batch)\n",
    "#     logger.debug(f\"loss: {loss}\")\n",
    "\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: dict,\n",
    "        lr: float = 1e-3,\n",
    "        num_epochs: int = 10\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.model = ColBERT(**params)\n",
    "\n",
    "    def forward(self, meal: Tensor) -> Any:\n",
    "        return self.model(meal)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, loss = self.model.trigger_train(**batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "        # scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.01, total_iters=self.num_epochs)\n",
    "        # return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        \n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(\"train\", queries, corpus, punctuations)\n",
    "loader = DataLoader(data, batch_size=int(conf[\"BSZ\"]), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'bert_model': conf['MODEL_NAME'],\n",
    "    'size_vocab': len(tokenizer),\n",
    "    'd_hid': conf['D_HID'],\n",
    "    'd_hid_bert': conf['D_HID_BERT'],\n",
    "}\n",
    "litmodel = LitModel(params, lr=float(conf['LR']), num_epochs=conf['NUM_EPOCHS'])\n",
    "# litmodel.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ckpt = Path(conf['PATHS']['ckpt'])\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    # devices=0,\n",
    "    callbacks=[\n",
    "        RichProgressBar(leave=True),\n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=path_ckpt / model_name,\n",
    "            filename=f\"{path_ckpt.stem}_{{epoch}}\",\n",
    "            every_n_epochs=2\n",
    "        )\n",
    "    ],\n",
    "    logger=TensorBoardLogger(conf['PATHS']['logs'], name=model_name, version=version, default_hp_metric=False),\n",
    "    # gradient_clip_val=1,\n",
    "    max_epochs=conf['NUM_EPOCHS'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    litmodel,\n",
    "    loader,\n",
    "    # ckpt_path='weights/embedding_tuning/stage-1_02-26_12-52-00_epoch=11.ckpt'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
