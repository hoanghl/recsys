{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from datetime import datetime\n",
    "\n",
    "import polars as pl\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "import einops\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import Module\n",
    "from torch import Tensor\n",
    "from polars import DataFrame\n",
    "from loguru import logger\n",
    "from lightning.pytorch.callbacks import RichProgressBar, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_DEBUG = False\n",
    "MODE = \"EVAL\"\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\" if IS_DEBUG else \"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"colbert/configs.yaml\"\n",
    "\n",
    "with open(path) as file:\n",
    "    conf = yaml.safe_load(file)\n",
    "\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(conf['PATHS']['tokenizer'])\n",
    "\n",
    "TOK_ID_MASK, TOK_ID_PAD = tokenizer.convert_tokens_to_ids(['[MASK]', '[PAD]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pl.read_parquet(conf['PROCESSED']['query'])\n",
    "queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw = Path(conf['PROCESSED']['corpus'].replace(\"[i]\", '*'))\n",
    "paths = Path(path_raw.parent).glob(path_raw.stem)\n",
    "\n",
    "if IS_DEBUG:\n",
    "    logger.debug(\"IS_DEBUG: Load part of corpus\")\n",
    "\n",
    "    corpus = pl.read_parquet(list(paths)[0])\n",
    "else:\n",
    "    logger.debug(\"Load full corpus\")\n",
    "\n",
    "    corpus = pl.concat([pl.read_parquet(path) for path in paths])\n",
    "\n",
    "corpus = corpus.with_columns(pl.col('did').cast(pl.Int64))\n",
    "\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "model_name = \"ColBERT\"\n",
    "\n",
    "with open(conf['PATHS']['punctuations']) as file:\n",
    "    map_punct2ids = json.load(file)\n",
    "\n",
    "punctuations = set(map_punct2ids.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataset and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: str,\n",
    "        queries: DataFrame,\n",
    "        corpus: DataFrame,\n",
    "        punctuations: set,\n",
    "        tok_id_mask: int,\n",
    "        tok_id_pad: int,\n",
    "        col_query_id: str = \"qid\",\n",
    "        col_corpus_id: str = \"did\",\n",
    "        col_tok_ids: str = \"tok_ids\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.split = split\n",
    "        self.punctuations = punctuations\n",
    "        self.queries = queries\n",
    "        self.col_query_id = col_query_id\n",
    "        self.col_corpus_id = col_corpus_id\n",
    "        self.col_tok_ids = col_tok_ids\n",
    "        self.tok_id_mask = tok_id_mask\n",
    "        self.tok_id_pad = tok_id_pad\n",
    "\n",
    "        pairs = Data._load_pairs(split)\n",
    "\n",
    "        # Reducing the size of corpus and positive pairs\n",
    "        self.corpus = corpus.join(\n",
    "            pairs.select(col_corpus_id).unique(), on=col_corpus_id, how=\"inner\"\n",
    "        )\n",
    "        self.pairs = pairs.join(corpus, on=col_corpus_id, how=\"inner\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        entry = self.pairs[index]\n",
    "        qid, did = entry[self.col_query_id].item(), entry[self.col_corpus_id].item()\n",
    "\n",
    "        tok_ids_query = (\n",
    "            self.queries\n",
    "            .filter(pl.col(self.col_query_id) == pl.lit(qid))[self.col_tok_ids]\n",
    "            .item()\n",
    "            .to_numpy()\n",
    "            .copy()\n",
    "        )  # fmt: skip\n",
    "        tok_ids_doc = (\n",
    "            self.corpus\n",
    "            .filter(pl.col(self.col_corpus_id) == did)[self.col_tok_ids]\n",
    "            .item()\n",
    "            .to_numpy()\n",
    "            .copy()\n",
    "        )  # fmt: skip\n",
    "        mask = torch.tensor(\n",
    "            [tok.item() in self.punctuations or tok.item() == self.tok_id_pad for tok in tok_ids_doc]\n",
    "        )\n",
    "\n",
    "        attention_mask_query = (tok_ids_query != self.tok_id_mask).astype(np.int32)\n",
    "        attention_mask_doc = (tok_ids_doc != self.tok_id_pad).astype(np.int32)\n",
    "\n",
    "        return {\n",
    "            \"query\": tok_ids_query,\n",
    "            \"attention_mask_query\": attention_mask_query,\n",
    "            \"attention_mask_doc\": attention_mask_doc,\n",
    "            \"doc\": tok_ids_doc,\n",
    "            \"mask\": mask,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_pairs(split: str) -> DataFrame:\n",
    "        \"\"\"Load positive pairs from train/val/tes split\n",
    "\n",
    "        Args:\n",
    "            split (str): split\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: positive pairs\n",
    "        \"\"\"\n",
    "\n",
    "        assert split in [\"train\", \"val\", \"test\"]\n",
    "        assert Path(conf[\"RAW_DATA\"][split]).exists()\n",
    "\n",
    "        pairs = pl.read_csv(conf[\"RAW_DATA\"][\"train\"], separator=\"\\t\").select(\n",
    "            pl.col(\"query-id\").alias(\"qid\"), pl.col(\"corpus-id\").alias(\"did\")\n",
    "        )\n",
    "\n",
    "        return pairs\n",
    "    \n",
    "# data = Data(\"val\", queries, corpus, punctuations, TOK_ID_MASK, TOK_ID_PAD)\n",
    "# # data[123]\n",
    "# loader = DataLoader(data, batch_size=int(conf[\"BSZ\"]), shuffle=True)\n",
    "# for batch in loader:\n",
    "#     break\n",
    "\n",
    "# batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColBERT(Module):\n",
    "    MAKS_VAL = -1e10\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model: str,\n",
    "        size_vocab: int,\n",
    "        d_hid: int = 128,\n",
    "        d_hid_bert: int = 768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.bert.resize_token_embeddings(size_vocab)\n",
    "\n",
    "        self.linear = nn.Linear(d_hid_bert, d_hid, bias=False)\n",
    "\n",
    "    def forward(self, X: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        # X: [bz, n]\n",
    "\n",
    "        X = self.bert(X, attention_mask).last_hidden_state\n",
    "        # [bz, n, d_hid_bert]\n",
    "\n",
    "        X = self.linear(X)\n",
    "        # [bz, n, d_hid]\n",
    "\n",
    "        # X = X / X.norm(dim=-1, keepdim=True)\n",
    "        X = torch.nn.functional.normalize(X, p=2, dim=-1)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def trigger_train(\n",
    "        self,\n",
    "        query: Tensor,\n",
    "        doc: Tensor,\n",
    "        mask: Tensor,\n",
    "        attention_mask_query: Tensor,\n",
    "        attention_mask_doc: Tensor,\n",
    "    ) -> Tensor:\n",
    "        # query: [bz, Nd]\n",
    "        # doc, mask: [bz, L]\n",
    "\n",
    "        bz, Nd = query.shape\n",
    "\n",
    "        ###################################################\n",
    "        # Encode query and document\n",
    "        ###################################################\n",
    "        query = self.forward(query, attention_mask_query)\n",
    "        # [bz, Nd, d_hid]\n",
    "        doc = self.forward(doc, attention_mask_doc)\n",
    "        # [bz, L, d_hid]\n",
    "\n",
    "        ###################################################\n",
    "        # Calculate the similarity\n",
    "        ###################################################\n",
    "        # Apply in-batch negative sampling\n",
    "        query = einops.repeat(query, \"b n d -> b repeat n d\", repeat=bz)\n",
    "\n",
    "        doc = einops.repeat(doc, \"b l d -> repeat b l d\", repeat=bz)\n",
    "        doc = einops.rearrange(doc, \"b a l d -> b a d l\")\n",
    "\n",
    "        sim = einops.einsum(query, doc, \"b a n d, b a d l -> b a n l\")\n",
    "        # [bz, bz, Nd, L]\n",
    "\n",
    "        # Mask positions which are the punctuation\n",
    "        mask = einops.repeat(mask, \"b l -> repeat1 b repeat2 l\", repeat1=bz, repeat2=Nd)\n",
    "        sim = sim.masked_fill(mask, ColBERT.MAKS_VAL)\n",
    "\n",
    "        # Calculate score\n",
    "        score = (sim.max(dim=-1).values).sum(dim=-1)\n",
    "        # [bz, bz]\n",
    "\n",
    "        logger.debug(f\"score = {score}\")\n",
    "\n",
    "        # Calculate Listwise CE\n",
    "        tgt = torch.arange(bz, dtype=torch.long, device=score.device)\n",
    "        # [bz]\n",
    "\n",
    "        loss = nn.functional.cross_entropy(score, tgt)\n",
    "\n",
    "        return score, loss\n",
    "\n",
    "# model = ColBERT(conf['MODEL_NAME'], len(tokenizer))\n",
    "# data = Data(\"train\", queries, corpus, punctuations)\n",
    "\n",
    "# loader = DataLoader(data, batch_size=int(conf[\"BSZ\"]), shuffle=True)\n",
    "# for batch in loader:\n",
    "#     score, loss = model.trigger_train(**batch)\n",
    "#     logger.debug(f\"loss: {loss}\")\n",
    "\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "\n",
    "class LitModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: dict,\n",
    "        lr: float = 1e-3,\n",
    "        num_epochs: int = 10,\n",
    "        use_lr_scheduler: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.use_lr_scheduler = use_lr_scheduler\n",
    "\n",
    "        self.model = ColBERT(**params)\n",
    "\n",
    "    def forward(self, meal: Tensor) -> Any:\n",
    "        return self.model(meal)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, loss = self.model.trigger_train(**batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        score, _ = self.model.trigger_train(**batch)\n",
    "        # score: [bz, bz]\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        bz = score.shape[0] \n",
    "        relevances = torch.eye(bz, dtype=score.dtype, device=self.device)\n",
    "\n",
    "        relevances_sorted = relevances[:, torch.argsort(score, dim=-1, descending=True)][0, :, : K]\n",
    "\n",
    "        val_ndcg = LitModel._calc_ndcg(relevances_sorted, K)\n",
    "        val_mrr = LitModel._calc_mrr(relevances_sorted, K)\n",
    "        val_map = LitModel._calc_map(relevances_sorted, K)\n",
    "\n",
    "        self.log(\"val_ndcg\", val_ndcg, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"val_mrr\", val_mrr, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"val_map\", val_map, on_epoch=True, prog_bar=False)\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "        out = {\"optimizer\": optimizer}\n",
    "\n",
    "        if self.use_lr_scheduler:\n",
    "            out[\"lr_scheduler\"] = torch.optim.lr_scheduler.LinearLR(\n",
    "                optimizer,\n",
    "                start_factor=1.0,\n",
    "                end_factor=0.01,\n",
    "                total_iters=self.num_epochs,\n",
    "            )\n",
    "\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calc_ndcg(relevances: Tensor, k: int) -> float:\n",
    "        indices = 1 / torch.log2(torch.arange(2, relevances.shape[-1] + 2, device=relevances.device)).unsqueeze(0)\n",
    "\n",
    "        ndcg = torch.mean(relevances @ indices.T).item()\n",
    "\n",
    "        return ndcg\n",
    "\n",
    "    @staticmethod\n",
    "    def _calc_mrr(relevances: Tensor, k: int) -> float:\n",
    "        vals = 1 / ((relevances.argmax(dim=-1) + 1) * relevances.max(dim=-1).values) \n",
    "        vals = vals.masked_fill(vals == torch.inf, 0)\n",
    "        mrr = torch.mean(vals)\n",
    "\n",
    "        return mrr\n",
    "\n",
    "    @staticmethod\n",
    "    def _calc_map(relevances: Tensor, k: int) -> float:\n",
    "        val_map = ((relevances.cumsum(dim=-1) / (torch.arange(relevances.shape[-1]) + 1) )).sum(-1).mean()\n",
    "        return val_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train or eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match MODE:\n",
    "    case \"TRAIN\":\n",
    "        split = \"train\"\n",
    "        shuffle = True\n",
    "    case \"EVAL\":\n",
    "        split = \"val\"\n",
    "        shuffle = False\n",
    "    case _:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "data = Data(split, queries, corpus, punctuations, TOK_ID_MASK, TOK_ID_PAD)\n",
    "loader = DataLoader(data, batch_size=int(conf[\"BSZ\"]), shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'bert_model': conf['MODEL_NAME'],\n",
    "    'size_vocab': len(tokenizer),\n",
    "    'd_hid': conf['D_HID'],\n",
    "    'd_hid_bert': conf['D_HID_BERT'],\n",
    "}\n",
    "litmodel = LitModel(params, lr=float(conf['LR']), num_epochs=conf['NUM_EPOCHS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ckpt = Path(conf['PATHS']['ckpt'])\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    # devices=0,\n",
    "    callbacks=[\n",
    "        RichProgressBar(leave=True),\n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=path_ckpt / model_name,\n",
    "            filename=f\"{path_ckpt.stem}_{{epoch}}\",\n",
    "            every_n_epochs=2\n",
    "        )\n",
    "    ],\n",
    "    logger=TensorBoardLogger(conf['PATHS']['logs'], name=model_name, version=version, default_hp_metric=False),\n",
    "    # gradient_clip_val=1,\n",
    "    max_epochs=conf['NUM_EPOCHS'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ckpt = Path(\"ckpts/ckpt_epoch=3.ckpt\")\n",
    "\n",
    "match MODE:\n",
    "    case \"TRAIN\":\n",
    "        trainer.fit(\n",
    "            litmodel,\n",
    "            loader,\n",
    "            ckpt_path=path_ckpt\n",
    "        )\n",
    "    case \"EVAL\":\n",
    "        assert path_ckpt.exists()\n",
    "\n",
    "        trainer.validate(\n",
    "            litmodel,\n",
    "            loader,\n",
    "            ckpt_path=path_ckpt\n",
    "        )\n",
    "    case _:\n",
    "        raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
